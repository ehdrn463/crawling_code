{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(n_url):\n",
    "    \"\"\" (string) -> list\n",
    "    Args:\n",
    "        n_url(string): 네이버 뉴스 url\n",
    "    Return:\n",
    "        news_detail(list)\n",
    "        해당 네이버 뉴스의 정보 (제목, 날짜, 본문, 언론사)가 담긴 list 반환\n",
    "    >>> get_news('https://www.edaily.co.kr/news/read?newsId=01305446625741368&mediaCodeNo=257&OutLnkChk=Y')\n",
    "    ['코로나가 아이들에게 가와사키병 일으킨다', '20200430', '기사 본문', '이데일리']    \n",
    "    \"\"\"\n",
    "    news_detail = []\n",
    "\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "\n",
    "    title = bsoup.select('h3#articleTitle')[0].text  #대괄호는  h3#articleTitle 인 것중 첫번째 그룹만 가져오겠다.\n",
    "    news_detail.append(title)\n",
    "\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate)\n",
    "\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip())\n",
    "  \n",
    "    news_detail.append(n_url)\n",
    "    \n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany)\n",
    "\n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(maxpage, query, product_list, disease_list, s_date, e_date):\n",
    "    \"\"\"(integer, string, string, string, string, string) -> (None)\n",
    "    Args:\n",
    "        maxpage(int): 크롤링할 최대 페이지 수\n",
    "        query(str): 네이버 뉴스 링크 검색 쿼리문 (검색 조건에 따른 키워드 조합)\n",
    "        product_list(str): 검색할 제품 단어\n",
    "        disease_list(str): 검색할 증상 단어\n",
    "        s_date(str): 크롤링할 시작 날짜\n",
    "        e_date(str): 크롤링할 끝 날짜\n",
    "    Return:\n",
    "        None\n",
    "        '기사 제목, 기사 배포 날짜, 기사 본문, 언론사'를 기록한 txt 파일 작성\n",
    "    >>> crwalwer(10, '자전거, 전동퀵보드', '부상, 골절상, 타박상', '2019.01.01', '2020.01.01')\n",
    "    '자전거, 전동퀵보드'+'부상, 골절상, 타박상'+'.txt' 파일 작성\n",
    "    \"\"\"\n",
    "    \n",
    "    # input 날짜 (2019.01.01)에서 '.' 제거\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    #maxpage_t: 입력한 maxpage에 따른 네이버 뉴스 기사 수 (네이버뉴스 검색 1page당 10개의 기사가 있음)\n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    f = open(RESULT_PATH + product_list + disease_list +\".txt\", 'w', encoding='utf-8')\n",
    "    \n",
    "    while page < maxpage_t:\n",
    "    \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        req = requests.get(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "\n",
    "        \n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                # 네이버뉴스링크가 존재하는 것만 수집\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    # get_news()함수를 통해 조건에 맞는 뉴스정보를 news_detail에 저장한 후 파일에 입력\n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "                    f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(product_list, disease_list, news_detail[1], news_detail[4], news_detail[0], news_detail[2], news_detail[3]))  # new style\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_make(product_list,disease_list):\n",
    "    \"\"\" (string, string) -> (None)\n",
    "    Args:\n",
    "        product_list(string) : 검색할 제품 단어\n",
    "        disease_list(string) : 검색할 증상 단어\n",
    "    Return:\n",
    "        None\n",
    "        crawling.txt -> crawling.xlsx 전환\n",
    "    >>> excel_make('자전거, 전동퀵보드', '부상, 골절상, 타박상')\n",
    "    '자전거, 전동퀵보드'+'부상, 골절상, 타박상'+'.csv' 파일 작성 (txt -> csv)\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(RESULT_PATH + product_list + disease_list + \".txt\", sep='\\t',header=None, error_bad_lines=False,engine='python', encoding='utf-8' )\n",
    "    # pandas dataframe column명 설정\n",
    "    data.columns = ['product_list','disease_list','years','company','title','contents','link']\n",
    "    \n",
    "    \n",
    "    xlsx_outputFileName = '%s-%s-%s-%s-%s  %s시 %s분 %s초 result.xlsx' % (str(product_list), str(disease_list), now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    #엑셀 파일로 저장\n",
    "    data.to_excel(RESULT_PATH+xlsx_outputFileName, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 경로 설정\n",
    "    RESULT_PATH = 'C:/Users/goddo/바탕 화면/intern/working/result_data/'\n",
    "    while True:\n",
    "        \n",
    "        # 파일저장을 위한 시간 설정\n",
    "        now = datetime.now()\n",
    "        keep = input(\"0 입력시 수집 종료 \\n1 입력시 수집 시작\\n: \")\n",
    "        if (keep == \"0\"):\n",
    "            break\n",
    "        \n",
    "        # 제품명, 증상명, 쿼리문 초기화\n",
    "        product_list = list()\n",
    "        disease_list = list()\n",
    "        query = ''\n",
    "        maxpage = input(\"최대 출력할 페이지수 입력하시오: \")\n",
    "        \n",
    "        # 증상 입력 -> 제품 입력 순임\n",
    "        # 논리구조상 '제품 입력 -> 제품과 관련된 증상 입력'이 순서가 맞으나\n",
    "        # 네이버뉴스 검색어 설정 순서가 '증상(선택 단어)' -> '제품(필수 단어)' 순으로 나타남\n",
    "        while True:\n",
    "            disease = input(\"관련 증상 입력: \\nquit입력시 다음 단계로 진행: \")\n",
    "            if disease == 'quit':\n",
    "                break\n",
    "            disease_list.append(disease)\n",
    "        \n",
    "        # 뉴스 쿼리문 특성상 마지막 단어에서는 '특수문자가 빠짐'\n",
    "        for i, disease in enumerate(disease_list):\n",
    "            if (i != len(disease_list)-1):\n",
    "                query += disease+'+%7C+'\n",
    "            else:\n",
    "                query +=disease\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            product = input(\"제품 이름(꼭 들어가야 하는 단어) 입력: \\nquit 입력시 다음 단계로 진행: \")\n",
    "            if product == 'quit':\n",
    "                break\n",
    "            product_list.append(product)\n",
    "            \n",
    "        for i, product in enumerate(product_list):\n",
    "            query += '+%2B'+product\n",
    "\n",
    "\n",
    "        # 시작 날짜, 마지막 날짜를 input으로 받되, 기본값은 2019.01.01 ~ 2020.01.01\n",
    "        s_date = input(\"시작날짜 입력(Default: 2019.01.01): \") or \"2019.01.01\"  \n",
    "        e_date = input(\"끝  날짜 입력(Default: 2020.01.01): \") or \"2020.01.01\" \n",
    "\n",
    "        \n",
    "        # 파일저장을 위해 list를 string으로 형변환해준다.\n",
    "        product_list = str(product_list)\n",
    "        disease_list = str(disease_list)\n",
    "        \n",
    "        # input값에 따라 크롤링 시작\n",
    "        crawler(maxpage,query,product_list,disease_list, s_date,e_date) #검색된 네이버뉴스의 기사내용을 크롤링합니다. \n",
    "        \n",
    "        # 크롤링결과 만들어진 txt -> csv\n",
    "        excel_make(product_list,disease_list) #엑셀로 만들기     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 입력시 수집 종료 \n",
      "1 입력시 수집 시작10\n",
      "최대 출력할 페이지수 입력하시오: 10\n",
      "관련 증상 입력: \n",
      "quit입력시 다음 단계로 진행복통\n",
      "관련 증상 입력: \n",
      "quit입력시 다음 단계로 진행설사\n",
      "관련 증상 입력: \n",
      "quit입력시 다음 단계로 진행quit\n",
      "복통+%7C+설사\n",
      "제품 이름(꼭 들어가야 하는 단어) 입력: \n",
      "quit 입력시 다음 단계로 진행노로바이러스\n",
      "제품 이름(꼭 들어가야 하는 단어) 입력: \n",
      "quit 입력시 다음 단계로 진행quit\n",
      "노로바이러스\n",
      "시작날짜 입력(2019.01.01):\n",
      "끝날짜 입력(2019.04.28):\n",
      " \n",
      "1\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=1\n",
      "11\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=11\n",
      "21\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=21\n",
      "31\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=31\n",
      "41\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=41\n",
      "51\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=51\n",
      "61\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=61\n",
      "71\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=71\n",
      "81\n",
      "https://search.naver.com/search.naver?where=news&query=복통+%7C+설사+%2B노로바이러스&sort=0&ds=&de=&nso=so%3Ar%2Cp%3Afromto%2Ca%3A&start=81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 27: '\t' expected after '\"'\n",
      "Skipping line 15: Expected 9 fields in line 15, saw 18\n",
      "Skipping line 22: Expected 9 fields in line 22, saw 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 입력시 수집 종료 \n",
      "1 입력시 수집 시작0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
